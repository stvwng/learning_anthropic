{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2dac2a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "from statistics import mean\n",
    "import json\n",
    "client = Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e613048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic eval workflow\n",
    "\n",
    "# generate a dataset\n",
    "\n",
    "# \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b962f06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_message(role:str, content:str, messages:list[str]) -> None:\n",
    "    \"\"\"Add a message to the conversation history\n",
    "    \n",
    "    Args:\n",
    "        role (str): The role of the message sender, i.e. \"user\" or \"assistant\"\n",
    "        content (str): The content of the messagemessage\n",
    "        messages (list[str]): The list of messages in the conversation\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"content\": content\n",
    "    }\n",
    "    messages.append(params)\n",
    "    \n",
    "def get_response_text(\n",
    "    messages:list[str],\n",
    "    system_prompt:str=None,\n",
    "    model:str=\"claude-sonnet-4-0\", \n",
    "    max_tokens:int=1000, \n",
    "    client:Anthropic=client,\n",
    "    stop_sequences:list[str]=None) -> str:\n",
    "    \"\"\"Get the response text from the model\n",
    "    \n",
    "    Args:\n",
    "        messages (list[str]): The list of messages in the conversation\n",
    "        system_prompt (str): The system prompt to use for the response\n",
    "        model (str): The model to use for the response\n",
    "        max_tokens (int): The maximum number of tokens in the response\n",
    "        client (Anthropic): The Anthropic client\n",
    "        stop_sequences (list[str]): The list of stop sequences to use for the response\n",
    "        \n",
    "    Returns:\n",
    "        str: The response text from the model\n",
    "    \"\"\"\n",
    "    \n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"messages\": messages\n",
    "    }\n",
    "    \n",
    "    if system_prompt:\n",
    "        params[\"system\"] = system_prompt\n",
    "    \n",
    "    if stop_sequences:\n",
    "        params[\"stop_sequences\"] = stop_sequences\n",
    "    \n",
    "    response = client.messages.create(**params)\n",
    "    return response.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "730b4ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset():\n",
    "    prompt = \"\"\"\n",
    "    Generate an evaluation dataset for a prompt evaluation. The dataset will be used to evaluate prompts that generate Python, JSON, or Regex specifically for AWS-related tasks. Generate an array of JSON objects, each representing task that requires Python, JSON, or a Regex to complete.\n",
    "\n",
    "    Example output:\n",
    "    ```json\n",
    "    [\n",
    "    {\n",
    "        \"task\": \"Description of task\",\n",
    "    },\n",
    "    ...additional\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    * Focus on tasks that can be solved by writing a single Python function, a single JSON object, or a single regex\n",
    "    * Focus on tasks that do not require writing much code\n",
    "\n",
    "    Please generate 3 objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = []\n",
    "    add_message(\"user\", prompt, messages)\n",
    "    add_message(\"assistant\", \"```json\", messages) # this tells Claude to start generating the response after \"```json\"\n",
    "    text = get_response_text(messages, model=\"claude-haiku-4-5\", stop_sequences=[\"```\"])\n",
    "    return json.loads(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96991c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'task': 'Write a Python function that takes an AWS S3 bucket name and returns True if it follows AWS naming conventions (lowercase, 3-63 characters, no consecutive hyphens), False otherwise.'}, {'task': \"Create a JSON object that represents an AWS IAM policy allowing a user to read all objects from an S3 bucket named 'my-bucket'.\"}, {'task': 'Write a Regex pattern that matches valid AWS EC2 instance IDs (format: i-followed by 8 or 17 hexadecimal characters).'}]\n"
     ]
    }
   ],
   "source": [
    "dataset = generate_dataset()\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e43ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write dataset into a file called prompt_eval_dataset.json\n",
    "\n",
    "with open('prompt_eval_dataset.json', 'w') as f:\n",
    "    json.dump(dataset, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ea3ded3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt(test_case:dict) -> str:\n",
    "    \"\"\"\n",
    "    Run a prompt for a given test case\n",
    "    \n",
    "    Args:\n",
    "        test_case (dict): The test case to run\n",
    "        \n",
    "    Returns:\n",
    "        str: The response from the prompt\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    \n",
    "    Please solve the following task:\n",
    "    \n",
    "    {test_case[\"task\"]}\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = []\n",
    "    add_message(\"user\", prompt, messages)\n",
    "    add_message(\"assistant\", \"```json\", messages) # this tells Claude to start generating the response after \"```json\"\n",
    "    output = get_response_text(messages, model=\"claude-haiku-4-5\", stop_sequences=[\"```\"])\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d64221a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_by_model(test_case:dict, output:str) -> dict:\n",
    "    \"\"\"\n",
    "    Grade an output by a given model\n",
    "    \n",
    "    Args:\n",
    "        test_case (dict): The test case to grade\n",
    "        output (str): The output to grade\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing the evaluation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create evaluation prompt\n",
    "    eval_prompt = \"\"\"\n",
    "    You are an expert code reviewer. Evaluate this AI-generated solution.\n",
    "    \n",
    "    Task: {task}\n",
    "    Solution: {solution}\n",
    "    \n",
    "    Provide your evaluation as a structured JSON object with:\n",
    "    - \"strengths\": An array of 1-3 key strengths\n",
    "    - \"weaknesses\": An array of 1-3 key areas for improvement  \n",
    "    - \"reasoning\": A concise explanation of your assessment\n",
    "    - \"score\": A number between 1-10\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = []\n",
    "    add_message(\"user\", eval_prompt, messages)\n",
    "    add_message(\"assistant\", \"```json\", messages)\n",
    "    \n",
    "    eval_text = get_response_text(messages, stop_sequences=[\"```\"])\n",
    "    return json.loads(eval_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "874577c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_case(test_case:dict) -> dict:\n",
    "    \"\"\"\n",
    "    Runs and grades a test case\n",
    "    \n",
    "    Args:\n",
    "        test_case (dict): The test case to run\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing the output, score, and test case\n",
    "    \"\"\"\n",
    "    output = run_prompt(test_case)\n",
    "    \n",
    "    report_card = grade_by_model(test_case, output)\n",
    "    score = report_card[\"score\"]\n",
    "    reasoning = report_card[\"reasoning\"]\n",
    "    \n",
    "    return {\n",
    "        \"output\": output,\n",
    "        \"score\": score,\n",
    "        \"reasoning\": reasoning,\n",
    "        \"test_case\": test_case\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1629e62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval(dataset:list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Runs and grades a dataset of test cases\n",
    "    \n",
    "    Args:\n",
    "        dataset (list[dict]): A list of test cases\n",
    "        \n",
    "    Returns:\n",
    "        list[dict]: A list of dictionaries containing the output, score, and test case\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for test_case in dataset:\n",
    "        result = run_test_case(test_case)\n",
    "        results.append(result)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2440bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score: 3.6666666666666665\n",
      "--------------------------------\n",
      "[\n",
      "  {\n",
      "    \"output\": \"\\n{\\n  \\\"function\\\": \\\"is_valid_s3_bucket_name\\\",\\n  \\\"description\\\": \\\"Validates if a bucket name follows AWS S3 naming conventions\\\",\\n  \\\"implementation\\\": {\\n    \\\"language\\\": \\\"python\\\",\\n    \\\"code\\\": \\\"import re\\\\n\\\\ndef is_valid_s3_bucket_name(bucket_name: str) -> bool:\\\\n    \\\\\\\"\\\\\\\"\\\\\\\"\\\\n    Validates if a bucket name follows AWS S3 naming conventions.\\\\n    \\\\n    AWS S3 Bucket Naming Rules:\\\\n    - Must be between 3 and 63 characters long\\\\n    - Must start and end with a lowercase letter or number\\\\n    - Can only contain lowercase letters, numbers, and hyphens\\\\n    - Cannot contain consecutive hyphens (--)\\\\n    - Cannot be formatted as an IP address (e.g., 192.168.1.1)\\\\n    \\\\n    Args:\\\\n        bucket_name (str): The S3 bucket name to validate\\\\n        \\\\n    Returns:\\\\n        bool: True if valid, False otherwise\\\\n    \\\\\\\"\\\\\\\"\\\\\\\"\\\\n    \\\\n    # Check if bucket_name is a string\\\\n    if not isinstance(bucket_name, str):\\\\n        return False\\\\n    \\\\n    # Check length (3-63 characters)\\\\n    if len(bucket_name) < 3 or len(bucket_name) > 63:\\\\n        return False\\\\n    \\\\n    # Check if it contains only lowercase letters, numbers, and hyphens\\\\n    if not re.match(r'^[a-z0-9-]+$', bucket_name):\\\\n        return False\\\\n    \\\\n    # Check for consecutive hyphens\\\\n    if '--' in bucket_name:\\\\n        return False\\\\n    \\\\n    # Check that it starts and ends with a lowercase letter or number (not hyphen)\\\\n    if bucket_name[0] == '-' or bucket_name[-1] == '-':\\\\n        return False\\\\n    \\\\n    # Check that it's not formatted as an IP address\\\\n    if re.match(r'^(\\\\\\\\d{1,3}\\\\\\\\.){3}\\\\\\\\d{1,3}$', bucket_name):\\\\n        return False\\\\n    \\\\n    return True\\\\n\\\\n\\\\n# Test cases\\\\nif __name__ == \\\\\\\"__main__\\\\\\\":\\\\n    test_cases = [\\\\n        # Valid bucket names\\\\n        (\\\\\\\"my-bucket\\\\\\\", True),\\\\n        (\\\\\\\"mybucket123\\\\\\\", True),\\\\n        (\\\\\\\"a-b-c\\\\\\\", True),\\\\n        (\\\\\\\"test123bucket\\\\\\\", True),\\\\n        (\\\\\\\"bucket-with-hyphens\\\\\\\", True),\\\\n        (\\\\\\\"abc\\\\\\\", True),\\\\n        (\\\\\\\"a\\\\\\\" * 63, True),\\\\n        \\\\n        # Invalid bucket names\\\\n        (\\\\\\\"My-Bucket\\\\\\\", False),  # Contains uppercase\\\\n        (\\\\\\\"my_bucket\\\\\\\", False),  # Contains underscore\\\\n        (\\\\\\\"my--bucket\\\\\\\", False),  # Consecutive hyphens\\\\n        (\\\\\\\"-mybucket\\\\\\\", False),  # Starts with hyphen\\\\n        (\\\\\\\"mybucket-\\\\\\\", False),  # Ends with hyphen\\\\n        (\\\\\\\"ab\\\\\\\", False),  # Too short (less than 3 chars)\\\\n        (\\\\\\\"a\\\\\\\" * 64, False),  # Too long (more than 63 chars)\\\\n        (\\\\\\\"192.168.1.1\\\\\\\", False),  # IP address format\\\\n        (\\\\\\\"my bucket\\\\\\\", False),  # Contains space\\\\n        (\\\\\\\"my.bucket\\\\\\\", False),  # Contains period (not allowed per strict rules)\\\\n        (\\\\\\\"\\\\\\\", False),  # Empty string\\\\n        (123, False),  # Not a string\\\\n        (None, False),  # None type\\\\n    ]\\\\n    \\\\n    print(\\\\\\\"Testing S3 Bucket Name Validation:\\\\\\\")\\\\n    print(\\\\\\\"-\\\\\\\" * 50)\\\\n    \\\\n    all_passed = True\\\\n    for bucket_name, expected in test_cases:\\\\n        result = is_valid_s3_bucket_name(bucket_name)\\\\n        status = \\\\\\\"\\u2713 PASS\\\\\\\" if result == expected else \\\\\\\"\\u2717 FAIL\\\\\\\"\\\\n        if result != expecte\",\n",
      "    \"score\": 7,\n",
      "    \"reasoning\": \"The solution demonstrates solid understanding of the core problem and implements an appropriate algorithm. The code structure is clean and the logic is sound. However, it lacks robust input validation and comprehensive error handling that would be expected in production code. The solution works correctly for valid inputs but could be more defensive against edge cases.\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Write a Python function that takes an AWS S3 bucket name and returns True if it follows AWS naming conventions (lowercase, 3-63 characters, no consecutive hyphens), False otherwise.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"output\": \"\\n{\\n  \\\"Version\\\": \\\"2012-10-17\\\",\\n  \\\"Statement\\\": [\\n    {\\n      \\\"Effect\\\": \\\"Allow\\\",\\n      \\\"Action\\\": [\\n        \\\"s3:GetObject\\\",\\n        \\\"s3:GetObjectVersion\\\"\\n      ],\\n      \\\"Resource\\\": \\\"arn:aws:s3:::my-bucket/*\\\"\\n    },\\n    {\\n      \\\"Effect\\\": \\\"Allow\\\",\\n      \\\"Action\\\": [\\n        \\\"s3:ListBucket\\\",\\n        \\\"s3:GetBucketVersioning\\\"\\n      ],\\n      \\\"Resource\\\": \\\"arn:aws:s3:::my-bucket\\\"\\n    }\\n  ]\\n}\\n\",\n",
      "    \"score\": 0,\n",
      "    \"reasoning\": \"This evaluation cannot be completed meaningfully without both the task description and the solution code. The request format is correct, but the essential content needed for a technical code review is missing. A proper evaluation would require examining the actual implementation against the specific requirements.\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Create a JSON object that represents an AWS IAM policy allowing a user to read all objects from an S3 bucket named 'my-bucket'.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"output\": \"\\n{\\n  \\\"regex_pattern\\\": \\\"^i-[0-9a-f]{8}(?:[0-9a-f]{9})?$\\\",\\n  \\\"explanation\\\": \\\"Pattern breakdown:\\\\n- ^ = Start of string\\\\n- i- = Literal 'i-' prefix required for all EC2 instance IDs\\\\n- [0-9a-f]{8} = Exactly 8 hexadecimal characters (0-9, a-f)\\\\n- (?:[0-9a-f]{9})? = Optional group of 9 additional hexadecimal characters (makes it 17 total when present)\\\\n- $ = End of string\\\\n- Non-capturing group (?:...) used for optional segment to avoid unnecessary capture\\\",\\n  \\\"valid_examples\\\": [\\n    \\\"i-1234abcd\\\",\\n    \\\"i-abcdef01\\\",\\n    \\\"i-0123456789abcdef0\\\",\\n    \\\"i-1a2b3c4d5e6f7g8h9\\\"\\n  ],\\n  \\\"invalid_examples\\\": [\\n    \\\"i-123\\\",\\n    \\\"i-1234abcde\\\",\\n    \\\"i-1234abcd5e6f7g8h9i\\\",\\n    \\\"1234abcd\\\",\\n    \\\"i_1234abcd\\\",\\n    \\\"I-1234abcd\\\"\\n  ],\\n  \\\"javascript_usage\\\": \\\"const pattern = /^i-[0-9a-f]{8}(?:[0-9a-f]{9})?$/; const isValid = pattern.test('i-1234abcd');\\\",\\n  \\\"python_usage\\\": \\\"import re; pattern = r'^i-[0-9a-f]{8}(?:[0-9a-f]{9})?$'; is_valid = re.match(pattern, 'i-1234abcd')\\\",\\n  \\\"notes\\\": \\\"The pattern is case-sensitive and expects lowercase hex characters. If you need to match uppercase or mixed case, modify to: ^i-[0-9a-fA-F]{8}(?:[0-9a-fA-F]{9})?$\\\"\\n}\\n\",\n",
      "    \"score\": 4,\n",
      "    \"reasoning\": \"While the response correctly identifies that the task and solution parameters are missing (showing good attention to detail), it fails to provide any actionable value. A more helpful approach would be to either provide a template evaluation structure, offer to review a hypothetical example, or give general code review guidelines. The response is technically correct but not practically useful.\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Write a Regex pattern that matches valid AWS EC2 instance IDs (format: i-followed by 8 or 17 hexadecimal characters).\"\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "with open('prompt_eval_dataset.json', 'r') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# run the evaluation\n",
    "results = run_eval(dataset)\n",
    "average_score = mean([result[\"score\"] for result in results])\n",
    "\n",
    "print(f\"Average score: {average_score}\")\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "print(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a023e724",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
