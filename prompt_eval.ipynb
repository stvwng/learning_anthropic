{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dac2a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "from statistics import mean\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "client = Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e613048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic eval workflow\n",
    "\n",
    "# generate a dataset\n",
    "\n",
    "# \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b962f06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_message(role:str, content:str, messages:list[str]) -> None:\n",
    "    \"\"\"Add a message to the conversation history\n",
    "    \n",
    "    Args:\n",
    "        role (str): The role of the message sender, i.e. \"user\" or \"assistant\"\n",
    "        content (str): The content of the messagemessage\n",
    "        messages (list[str]): The list of messages in the conversation\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"content\": content\n",
    "    }\n",
    "    messages.append(params)\n",
    "    \n",
    "def get_response_text(\n",
    "    messages:list[str],\n",
    "    system_prompt:str=None,\n",
    "    model:str=\"claude-sonnet-4-0\", \n",
    "    max_tokens:int=1000, \n",
    "    client:Anthropic=client,\n",
    "    stop_sequences:list[str]=None) -> str:\n",
    "    \"\"\"Get the response text from the model\n",
    "    \n",
    "    Args:\n",
    "        messages (list[str]): The list of messages in the conversation\n",
    "        system_prompt (str): The system prompt to use for the response\n",
    "        model (str): The model to use for the response\n",
    "        max_tokens (int): The maximum number of tokens in the response\n",
    "        client (Anthropic): The Anthropic client\n",
    "        stop_sequences (list[str]): The list of stop sequences to use for the response\n",
    "        \n",
    "    Returns:\n",
    "        str: The response text from the model\n",
    "    \"\"\"\n",
    "    \n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"messages\": messages\n",
    "    }\n",
    "    \n",
    "    if system_prompt:\n",
    "        params[\"system\"] = system_prompt\n",
    "    \n",
    "    if stop_sequences:\n",
    "        params[\"stop_sequences\"] = stop_sequences\n",
    "    \n",
    "    response = client.messages.create(**params)\n",
    "    return response.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "730b4ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset():\n",
    "    prompt = \"\"\"\n",
    "    Generate an evaluation dataset for a prompt evaluation. The dataset will be used to evaluate prompts that generate Python, JSON, or Regex specifically for AWS-related tasks. Generate an array of JSON objects, each representing task that requires Python, JSON, or a Regex to complete.\n",
    "\n",
    "    Example output:\n",
    "    ```json\n",
    "    [\n",
    "    {\n",
    "        \"task\": \"Description of task\",\n",
    "        \"format\": \"json\" or \"python\" or \"regex\"\n",
    "    },\n",
    "    ...additional\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    * Focus on tasks that can be solved by writing a single Python function, a single JSON object, or a single regex\n",
    "    * Focus on tasks that do not require writing much code\n",
    "\n",
    "    Please generate 3 objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = []\n",
    "    add_message(\"user\", prompt, messages)\n",
    "    add_message(\"assistant\", \"```json\", messages) # this tells Claude to start generating the response after \"```json\"\n",
    "    text = get_response_text(messages, model=\"claude-haiku-4-5\", stop_sequences=[\"```\"])\n",
    "    return json.loads(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96991c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'task': 'Create a JSON configuration object for an AWS Lambda function that processes S3 events with a 5-minute timeout and 512MB memory allocation', 'format': 'json'}, {'task': 'Write a Python function that takes an AWS IAM policy document (as a dictionary) and returns a list of all the resource ARNs that are granted access', 'format': 'python'}, {'task': 'Create a regex pattern that validates AWS S3 bucket names according to AWS naming rules (3-63 characters, lowercase letters, numbers, and hyphens, must start and end with alphanumeric)', 'format': 'regex'}]\n"
     ]
    }
   ],
   "source": [
    "dataset = generate_dataset()\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4e43ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write dataset into a file called prompt_eval_dataset.json\n",
    "\n",
    "with open('prompt_eval_dataset.json', 'w') as f:\n",
    "    json.dump(dataset, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ea3ded3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt(test_case:dict) -> str:\n",
    "    \"\"\"\n",
    "    Run a prompt for a given test case\n",
    "    \n",
    "    Args:\n",
    "        test_case (dict): The test case to run\n",
    "        \n",
    "    Returns:\n",
    "        str: The response from the prompt\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    \n",
    "    Please solve the following task:\n",
    "    \n",
    "    {test_case[\"task\"]}\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = []\n",
    "    add_message(\"user\", prompt, messages)\n",
    "    add_message(\"assistant\", \"```code\", messages) # this tells Claude to start generating the response after \"```code\"\n",
    "    output = get_response_text(messages, model=\"claude-haiku-4-5\", stop_sequences=[\"```\"])\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d64221a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_by_model(test_case:dict, output:str) -> dict:\n",
    "    \"\"\"\n",
    "    Grade an output by a given model\n",
    "    \n",
    "    Args:\n",
    "        test_case (dict): The test case to grade\n",
    "        output (str): The output to grade\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing the evaluation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create evaluation prompt\n",
    "    eval_prompt = \"\"\"\n",
    "    You are an expert code reviewer. Evaluate this AI-generated solution.\n",
    "    \n",
    "    Task: {task}\n",
    "    Solution: {solution}\n",
    "    \n",
    "    Provide your evaluation as a structured JSON object with:\n",
    "    - \"strengths\": An array of 1-3 key strengths\n",
    "    - \"weaknesses\": An array of 1-3 key areas for improvement  \n",
    "    - \"reasoning\": A concise explanation of your assessment\n",
    "    - \"score\": A number between 1-10\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = []\n",
    "    add_message(\"user\", eval_prompt, messages)\n",
    "    add_message(\"assistant\", \"```json\", messages)\n",
    "    \n",
    "    eval_text = get_response_text(messages, stop_sequences=[\"```\"])\n",
    "    return json.loads(eval_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f935799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code (or syntax) grader functions\n",
    "\n",
    "def validate_json(text):\n",
    "    try:\n",
    "        json.loads(text.strip())\n",
    "        return 10\n",
    "    except json.JSONDecodeError:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def validate_python(text):\n",
    "    try:\n",
    "        ast.parse(text.strip())\n",
    "        return 10\n",
    "    except SyntaxError:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def validate_regex(text):\n",
    "    try:\n",
    "        re.compile(text.strip())\n",
    "        return 10\n",
    "    except re.error:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def grade_syntax(response, test_case):\n",
    "    format = test_case[\"format\"]\n",
    "    if format == \"json\":\n",
    "        return validate_json(response)\n",
    "    elif format == \"python\":\n",
    "        return validate_python(response)\n",
    "    else:\n",
    "        return validate_regex(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "874577c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_case(test_case:dict) -> dict:\n",
    "    \"\"\"\n",
    "    Runs and grades a test case\n",
    "    \n",
    "    Args:\n",
    "        test_case (dict): The test case to run\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing the output, score, and test case\n",
    "    \"\"\"\n",
    "    output = run_prompt(test_case)\n",
    "    \n",
    "    model_grade = grade_by_model(test_case, output)\n",
    "    model_score = model_grade[\"score\"]\n",
    "    reasoning = model_grade[\"reasoning\"]\n",
    "    \n",
    "    syntax_score = grade_syntax(output, test_case)\n",
    "    \n",
    "    score = (model_score + syntax_score) / 2\n",
    "    return {\n",
    "        \"output\": output,\n",
    "        \"score\": score,\n",
    "        \"reasoning\": reasoning,\n",
    "        \"test_case\": test_case\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1629e62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval(dataset:list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Runs and grades a dataset of test cases\n",
    "    \n",
    "    Args:\n",
    "        dataset (list[dict]): A list of test cases\n",
    "        \n",
    "    Returns:\n",
    "        list[dict]: A list of dictionaries containing the output, score, and test case\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for test_case in dataset:\n",
    "        result = run_test_case(test_case)\n",
    "        results.append(result)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2440bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score: 6.166666666666667\n",
      "--------------------------------\n",
      "[\n",
      "  {\n",
      "    \"output\": \"\\n{\\n  \\\"FunctionName\\\": \\\"s3-event-processor\\\",\\n  \\\"Runtime\\\": \\\"nodejs18.x\\\",\\n  \\\"Role\\\": \\\"arn:aws:iam::ACCOUNT_ID:role/lambda-s3-execution-role\\\",\\n  \\\"Handler\\\": \\\"index.handler\\\",\\n  \\\"Timeout\\\": 300,\\n  \\\"MemorySize\\\": 512,\\n  \\\"Description\\\": \\\"Lambda function that processes S3 events\\\",\\n  \\\"Environment\\\": {\\n    \\\"Variables\\\": {\\n      \\\"LOG_LEVEL\\\": \\\"INFO\\\"\\n    }\\n  },\\n  \\\"Code\\\": {\\n    \\\"ZipFile\\\": \\\"async function handler(event) {\\\\n  console.log('S3 Event:', JSON.stringify(event, null, 2));\\\\n  for (const record of event.Records) {\\\\n    const bucket = record.s3.bucket.name;\\\\n    const key = decodeURIComponent(record.s3.object.key.replace(/\\\\\\\\+/g, ' '));\\\\n    console.log(`Processing s3://${bucket}/${key}`);\\\\n  }\\\\n  return { statusCode: 200, body: 'Events processed successfully' };\\\\n}\\\\nmodule.exports = { handler };\\\"\\n  },\\n  \\\"EphemeralStorage\\\": {\\n    \\\"Size\\\": 512\\n  },\\n  \\\"Layers\\\": [],\\n  \\\"TracingConfig\\\": {\\n    \\\"Mode\\\": \\\"Active\\\"\\n  },\\n  \\\"EventInvokeConfig\\\": {\\n    \\\"MaximumEventAge\\\": 3600,\\n    \\\"MaximumRetryAttempts\\\": 2,\\n    \\\"DestinationConfig\\\": {\\n      \\\"OnSuccess\\\": {\\n        \\\"Type\\\": \\\"SNS\\\",\\n        \\\"Destination\\\": \\\"arn:aws:sns:REGION:ACCOUNT_ID:success-topic\\\"\\n      },\\n      \\\"OnFailure\\\": {\\n        \\\"Type\\\": \\\"SNS\\\",\\n        \\\"Destination\\\": \\\"arn:aws:sns:REGION:ACCOUNT_ID:failure-topic\\\"\\n      }\\n    }\\n  }\\n}\\n\",\n",
      "    \"score\": 5.0,\n",
      "    \"reasoning\": \"I cannot provide a meaningful evaluation without seeing the actual task description and solution code. To properly review code, I need to assess correctness, efficiency, readability, maintainability, and adherence to requirements - all of which require the specific implementation details.\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Create a JSON configuration object for an AWS Lambda function that processes S3 events with a 5-minute timeout and 512MB memory allocation\",\n",
      "      \"format\": \"json\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"output\": \"\\ndef extract_resource_arns(policy_document):\\n    \\\"\\\"\\\"\\n    Extracts all resource ARNs from an AWS IAM policy document.\\n    \\n    Args:\\n        policy_document (dict): An AWS IAM policy document as a dictionary\\n        \\n    Returns:\\n        list: A list of all unique resource ARNs granted access in the policy\\n        \\n    Example:\\n        >>> policy = {\\n        ...     \\\"Version\\\": \\\"2012-10-17\\\",\\n        ...     \\\"Statement\\\": [\\n        ...         {\\n        ...             \\\"Effect\\\": \\\"Allow\\\",\\n        ...             \\\"Action\\\": \\\"s3:GetObject\\\",\\n        ...             \\\"Resource\\\": \\\"arn:aws:s3:::my-bucket/*\\\"\\n        ...         },\\n        ...         {\\n        ...             \\\"Effect\\\": \\\"Allow\\\",\\n        ...             \\\"Action\\\": \\\"dynamodb:*\\\",\\n        ...             \\\"Resource\\\": [\\n        ...                 \\\"arn:aws:dynamodb:us-east-1:123456789012:table/MyTable\\\",\\n        ...                 \\\"arn:aws:dynamodb:us-east-1:123456789012:table/MyTable/index/*\\\"\\n        ...             ]\\n        ...         }\\n        ...     ]\\n        ... }\\n        >>> extract_resource_arns(policy)\\n        ['arn:aws:s3:::my-bucket/*', \\n         'arn:aws:dynamodb:us-east-1:123456789012:table/MyTable',\\n         'arn:aws:dynamodb:us-east-1:123456789012:table/MyTable/index/*']\\n    \\\"\\\"\\\"\\n    arns = []\\n    \\n    # Handle empty or invalid policy documents\\n    if not policy_document or not isinstance(policy_document, dict):\\n        return arns\\n    \\n    # Extract statements - it should be a list\\n    statements = policy_document.get(\\\"Statement\\\", [])\\n    \\n    if not isinstance(statements, list):\\n        return arns\\n    \\n    # Iterate through each statement\\n    for statement in statements:\\n        if not isinstance(statement, dict):\\n            continue\\n            \\n        # Get the Resource field - can be a string or list of strings\\n        resource = statement.get(\\\"Resource\\\")\\n        \\n        if resource:\\n            # If Resource is a string, add it\\n            if isinstance(resource, str):\\n                if resource not in arns:  # Avoid duplicates\\n                    arns.append(resource)\\n            # If Resource is a list, add each element\\n            elif isinstance(resource, list):\\n                for arn in resource:\\n                    if isinstance(arn, str) and arn not in arns:\\n                        arns.append(arn)\\n    \\n    return arns\\n\\n\\n# Test cases\\nif __name__ == \\\"__main__\\\":\\n    # Test 1: Basic policy with single resource\\n    policy1 = {\\n        \\\"Version\\\": \\\"2012-10-17\\\",\\n        \\\"Statement\\\": [\\n            {\\n                \\\"Effect\\\": \\\"Allow\\\",\\n                \\\"Action\\\": \\\"s3:GetObject\\\",\\n                \\\"Resource\\\": \\\"arn:aws:s3:::my-bucket/*\\\"\\n            }\\n        ]\\n    }\\n    print(\\\"Test 1:\\\", extract_resource_arns(policy1))\\n    # Output: ['arn:aws:s3:::my-bucket/*']\\n    \\n    # Test 2: Policy with multiple resources in list\\n    policy2 = {\\n        \\\"Version\\\": \\\"2012-10-17\\\",\\n        \\\"Statement\\\": [\\n            {\\n                \\\"Effect\\\": \\\"Allow\\\",\\n                \\\"Action\\\": \\\"dynamodb:*\\\",\\n                \\\"Resource\\\": [\\n                    \\\"arn:aws:dynamodb:us-east-1:123456789012:table/MyTable\\\",\\n                    \\\"arn:aws:dynamodb:us-east-1:123456789012:table/MyTable/index/*\\\"\\n                ]\\n            }\\n        ]\\n    }\\n    print(\\\"Test 2:\\\", extract_resource_arns(policy2))\\n    # Output: ['arn:aws:dynamodb:us-east-1:123456789012:table/MyTable', \\n    #          'arn:aws:dynamodb:us-east-1:123456789012:table/MyTable/index/*']\\n    \",\n",
      "    \"score\": 5.0,\n",
      "    \"reasoning\": \"This evaluation cannot be completed as no specific task description or solution code was provided in the input. To perform a meaningful code review, I would need to see the actual problem statement and the AI-generated code solution to assess factors like correctness, efficiency, readability, error handling, and adherence to best practices.\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Write a Python function that takes an AWS IAM policy document (as a dictionary) and returns a list of all the resource ARNs that are granted access\",\n",
      "      \"format\": \"python\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"output\": \"\\n^[a-z0-9]([a-z0-9-]{1,61}[a-z0-9])?$\\n\",\n",
      "    \"score\": 8.5,\n",
      "    \"reasoning\": \"This is a solid implementation of Floyd's tortoise and hare algorithm for cycle detection. The core logic is correct and efficient with O(n) time complexity and O(1) space complexity. However, it needs better error handling for edge cases and could be more self-documenting. The algorithm will correctly identify cycles by having the fast pointer eventually 'lap' the slow pointer if a cycle exists.\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Create a regex pattern that validates AWS S3 bucket names according to AWS naming rules (3-63 characters, lowercase letters, numbers, and hyphens, must start and end with alphanumeric)\",\n",
      "      \"format\": \"regex\"\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "with open('prompt_eval_dataset.json', 'r') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# run the evaluation\n",
    "results = run_eval(dataset)\n",
    "average_score = mean([result[\"score\"] for result in results])\n",
    "\n",
    "print(f\"Average score: {average_score}\")\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "print(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a023e724",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
