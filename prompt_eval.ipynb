{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dac2a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "from statistics import mean\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "client = Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e613048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic eval workflow\n",
    "\n",
    "# generate a dataset\n",
    "\n",
    "# \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b962f06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_message(role:str, content:str, messages:list[str]) -> None:\n",
    "    \"\"\"Add a message to the conversation history\n",
    "    \n",
    "    Args:\n",
    "        role (str): The role of the message sender, i.e. \"user\" or \"assistant\"\n",
    "        content (str): The content of the messagemessage\n",
    "        messages (list[str]): The list of messages in the conversation\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"role\": role,\n",
    "        \"content\": content\n",
    "    }\n",
    "    messages.append(params)\n",
    "    \n",
    "def get_response_text(\n",
    "    messages:list[str],\n",
    "    system_prompt:str=None,\n",
    "    model:str=\"claude-sonnet-4-0\", \n",
    "    max_tokens:int=1000, \n",
    "    client:Anthropic=client,\n",
    "    stop_sequences:list[str]=None) -> str:\n",
    "    \"\"\"Get the response text from the model\n",
    "    \n",
    "    Args:\n",
    "        messages (list[str]): The list of messages in the conversation\n",
    "        system_prompt (str): The system prompt to use for the response\n",
    "        model (str): The model to use for the response\n",
    "        max_tokens (int): The maximum number of tokens in the response\n",
    "        client (Anthropic): The Anthropic client\n",
    "        stop_sequences (list[str]): The list of stop sequences to use for the response\n",
    "        \n",
    "    Returns:\n",
    "        str: The response text from the model\n",
    "    \"\"\"\n",
    "    \n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"messages\": messages\n",
    "    }\n",
    "    \n",
    "    if system_prompt:\n",
    "        params[\"system\"] = system_prompt\n",
    "    \n",
    "    if stop_sequences:\n",
    "        params[\"stop_sequences\"] = stop_sequences\n",
    "    \n",
    "    response = client.messages.create(**params)\n",
    "    return response.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "730b4ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset():\n",
    "    prompt = \"\"\"\n",
    "    Generate a evaluation dataset for a prompt evaluation. The dataset will be used to evaluate prompts\n",
    "    that generate Python, JSON, or Regex specifically for AWS-related tasks. Generate an array of JSON objects,\n",
    "    each representing task that requires Python, JSON, or a Regex to complete.\n",
    "\n",
    "    Example output:\n",
    "    ```json\n",
    "    [\n",
    "        {\n",
    "            \"task\": \"Description of task\",\n",
    "            \"format\": \"json\" or \"python\" or \"regex\",\n",
    "            \"solution_criteria\": \"Key criteria for evaluating the solution\"\n",
    "        },\n",
    "        ...additional\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    * Focus on tasks that can be solved by writing a single Python function, a single JSON object, or a regular expression.\n",
    "    * Focus on tasks that do not require writing much code\n",
    "\n",
    "    Please generate 3 objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = []\n",
    "    add_message(\"user\", prompt, messages)\n",
    "    add_message(\"assistant\", \"```json\", messages) # this tells Claude to start generating the response after \"```json\"\n",
    "    text = get_response_text(messages, model=\"claude-haiku-4-5\", stop_sequences=[\"```\"])\n",
    "    return json.loads(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96991c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'task': \"Parse an AWS S3 bucket name from a full S3 URI (e.g., 's3://my-bucket-name/path/to/file.txt') and extract only the bucket name\", 'format': 'regex', 'solution_criteria': 'Must correctly extract bucket names from various S3 URI formats, handle URIs with and without trailing paths, and account for bucket naming rules (lowercase, hyphens, numbers)'}, {'task': 'Create a JSON configuration object for an AWS Lambda function that includes environment variables, memory allocation, timeout, and a basic IAM role ARN', 'format': 'json', 'solution_criteria': 'JSON must be valid, include required Lambda configuration fields (FunctionName, Runtime, Handler, Role, Timeout, MemorySize), contain at least 2 environment variables, and follow AWS Lambda CloudFormation or API schema'}, {'task': 'Write a Python function that validates an AWS IAM role ARN string and returns True if valid, False otherwise', 'format': 'python', 'solution_criteria': 'Function must validate ARN format (arn:aws:iam::account-id:role/role-name), check account ID is numeric, ensure role name contains only valid characters, and handle edge cases like empty strings'}]\n"
     ]
    }
   ],
   "source": [
    "dataset = generate_dataset()\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4e43ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write dataset into a file called prompt_eval_dataset.json\n",
    "\n",
    "with open('prompt_eval_dataset.json', 'w') as f:\n",
    "    json.dump(dataset, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ea3ded3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt(test_case:dict) -> str:\n",
    "    \"\"\"\n",
    "    Run a prompt for a given test case\n",
    "    \n",
    "    Args:\n",
    "        test_case (dict): The test case to run\n",
    "        \n",
    "    Returns:\n",
    "        str: The response from the prompt\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    \n",
    "    Please solve the following task:\n",
    "    \n",
    "    {test_case[\"task\"]}\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = []\n",
    "    add_message(\"user\", prompt, messages)\n",
    "    add_message(\"assistant\", \"```code\", messages) # this tells Claude to start generating the response after \"```code\"\n",
    "    output = get_response_text(messages, model=\"claude-haiku-4-5\", stop_sequences=[\"```\"])\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d64221a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_by_model(test_case:dict, output:str) -> dict:\n",
    "    \"\"\"\n",
    "    Grade an output by a given model\n",
    "    \n",
    "    Args:\n",
    "        test_case (dict): The test case to grade\n",
    "        output (str): The output to grade\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing the evaluation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create evaluation prompt\n",
    "    eval_prompt = f\"\"\"\n",
    "    You are an expert AWS code reviewer. Your task is to evaluate the following AI-generated solution.\n",
    "\n",
    "    Original Task:\n",
    "    <task>\n",
    "    {test_case[\"task\"]}\n",
    "    </task>\n",
    "\n",
    "    Solution to Evaluate:\n",
    "    <solution>\n",
    "    {output}\n",
    "    </solution>\n",
    "    \n",
    "    Criteria for evaluation:\n",
    "    <criteria>\n",
    "    {test_case[\"solution_criteria\"]}\n",
    "    </criteria>\n",
    "\n",
    "    Output Format\n",
    "    Provide your evaluation as a structured JSON object with the following fields, in this specific order:\n",
    "    - \"strengths\": An array of 1-3 key strengths\n",
    "    - \"weaknesses\": An array of 1-3 key areas for improvement\n",
    "    - \"reasoning\": A concise explanation of your overall assessment\n",
    "    - \"score\": A number between 1-10\n",
    "\n",
    "    Respond with JSON. Keep your response concise and direct.\n",
    "    Example response shape:\n",
    "    {{\n",
    "        \"strengths\": string[],\n",
    "        \"weaknesses\": string[],\n",
    "        \"reasoning\": string,\n",
    "        \"score\": number\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = []\n",
    "    add_message(\"user\", eval_prompt, messages)\n",
    "    add_message(\"assistant\", \"```json\", messages)\n",
    "    \n",
    "    eval_text = get_response_text(messages, stop_sequences=[\"```\"])\n",
    "    return json.loads(eval_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f935799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code (or syntax) grader functions\n",
    "\n",
    "def validate_json(text):\n",
    "    try:\n",
    "        json.loads(text.strip())\n",
    "        return 10\n",
    "    except json.JSONDecodeError:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def validate_python(text):\n",
    "    try:\n",
    "        ast.parse(text.strip())\n",
    "        return 10\n",
    "    except SyntaxError:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def validate_regex(text):\n",
    "    try:\n",
    "        re.compile(text.strip())\n",
    "        return 10\n",
    "    except re.error:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def grade_syntax(response, test_case):\n",
    "    format = test_case[\"format\"]\n",
    "    if format == \"json\":\n",
    "        return validate_json(response)\n",
    "    elif format == \"python\":\n",
    "        return validate_python(response)\n",
    "    else:\n",
    "        return validate_regex(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "874577c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_case(test_case:dict) -> dict:\n",
    "    \"\"\"\n",
    "    Runs and grades a test case\n",
    "    \n",
    "    Args:\n",
    "        test_case (dict): The test case to run\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing the output, score, and test case\n",
    "    \"\"\"\n",
    "    output = run_prompt(test_case)\n",
    "    \n",
    "    model_grade = grade_by_model(test_case, output)\n",
    "    model_score = model_grade[\"score\"]\n",
    "    reasoning = model_grade[\"reasoning\"]\n",
    "    \n",
    "    syntax_score = grade_syntax(output, test_case)\n",
    "    \n",
    "    score = (model_score + syntax_score) / 2\n",
    "    return {\n",
    "        \"output\": output,\n",
    "        \"score\": score,\n",
    "        \"reasoning\": reasoning,\n",
    "        \"test_case\": test_case\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1629e62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval(dataset:list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Runs and grades a dataset of test cases\n",
    "    \n",
    "    Args:\n",
    "        dataset (list[dict]): A list of test cases\n",
    "        \n",
    "    Returns:\n",
    "        list[dict]: A list of dictionaries containing the output, score, and test case\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for test_case in dataset:\n",
    "        result = run_test_case(test_case)\n",
    "        results.append(result)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2440bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score: 8.666666666666666\n",
      "--------------------------------\n",
      "[\n",
      "  {\n",
      "    \"output\": \"\\nfunction parseS3BucketName(s3Uri) {\\n  // Remove 's3://' prefix and extract bucket name\\n  const bucketName = s3Uri.replace('s3://', '').split('/')[0];\\n  return bucketName;\\n}\\n\\n// Examples\\nconsole.log(parseS3BucketName('s3://my-bucket-name/path/to/file.txt'));\\n// Output: my-bucket-name\\n\\nconsole.log(parseS3BucketName('s3://another-bucket/folder/document.pdf'));\\n// Output: another-bucket\\n\\nconsole.log(parseS3BucketName('s3://simple-bucket'));\\n// Output: simple-bucket\\n\",\n",
      "    \"score\": 8.0,\n",
      "    \"reasoning\": \"The solution correctly implements the basic functionality and handles the provided test cases well. However, it lacks robustness for production use due to missing input validation and error handling. While it meets the core requirement, it doesn't account for edge cases or invalid inputs that could cause runtime errors.\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Parse an AWS S3 bucket name from a full S3 URI (e.g., 's3://my-bucket-name/path/to/file.txt') and extract only the bucket name\",\n",
      "      \"format\": \"regex\",\n",
      "      \"solution_criteria\": \"Must correctly extract bucket names from various S3 URI formats, handle URIs with and without trailing paths, and account for bucket naming rules (lowercase, hyphens, numbers)\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"output\": \"\\n{\\n  \\\"functionName\\\": \\\"my-lambda-function\\\",\\n  \\\"runtime\\\": \\\"nodejs18.x\\\",\\n  \\\"handler\\\": \\\"index.handler\\\",\\n  \\\"role\\\": \\\"arn:aws:iam::123456789012:role/lambda-execution-role\\\",\\n  \\\"memorySize\\\": 256,\\n  \\\"timeout\\\": 30,\\n  \\\"environment\\\": {\\n    \\\"variables\\\": {\\n      \\\"DB_HOST\\\": \\\"mydb.example.com\\\",\\n      \\\"DB_PORT\\\": \\\"5432\\\",\\n      \\\"DB_NAME\\\": \\\"myapp_db\\\",\\n      \\\"API_KEY\\\": \\\"${ssm:/myapp/api-key}\\\",\\n      \\\"ENVIRONMENT\\\": \\\"production\\\",\\n      \\\"LOG_LEVEL\\\": \\\"info\\\",\\n      \\\"TABLE_NAME\\\": \\\"users-table\\\"\\n    }\\n  },\\n  \\\"description\\\": \\\"Lambda function for processing user data\\\",\\n  \\\"architectures\\\": [\\\"x86_64\\\"],\\n  \\\"ephemeralStorage\\\": {\\n    \\\"size\\\": 512\\n  },\\n  \\\"tracingConfig\\\": {\\n    \\\"mode\\\": \\\"Active\\\"\\n  },\\n  \\\"tags\\\": {\\n    \\\"project\\\": \\\"myapp\\\",\\n    \\\"environment\\\": \\\"prod\\\",\\n    \\\"team\\\": \\\"backend\\\"\\n  }\\n}\\n\",\n",
      "    \"score\": 9.0,\n",
      "    \"reasoning\": \"The solution excellently meets all criteria with valid JSON, required fields, and multiple environment variables. It demonstrates AWS best practices with security considerations, monitoring, and resource management. Minor issues include placeholder values and potential schema ambiguity.\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Create a JSON configuration object for an AWS Lambda function that includes environment variables, memory allocation, timeout, and a basic IAM role ARN\",\n",
      "      \"format\": \"json\",\n",
      "      \"solution_criteria\": \"JSON must be valid, include required Lambda configuration fields (FunctionName, Runtime, Handler, Role, Timeout, MemorySize), contain at least 2 environment variables, and follow AWS Lambda CloudFormation or API schema\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"output\": \"\\nimport re\\n\\ndef validate_iam_role_arn(arn: str) -> bool:\\n    \\\"\\\"\\\"\\n    Validates an AWS IAM role ARN string.\\n    \\n    Args:\\n        arn: The ARN string to validate\\n        \\n    Returns:\\n        True if the ARN is a valid IAM role ARN, False otherwise\\n        \\n    Example:\\n        >>> validate_iam_role_arn(\\\"arn:aws:iam::123456789012:role/MyRole\\\")\\n        True\\n        >>> validate_iam_role_arn(\\\"arn:aws:iam::123456789012:role/path/MyRole\\\")\\n        True\\n        >>> validate_iam_role_arn(\\\"arn:aws:iam::123456789012:user/MyUser\\\")\\n        False\\n        >>> validate_iam_role_arn(\\\"invalid-arn\\\")\\n        False\\n    \\\"\\\"\\\"\\n    \\n    if not isinstance(arn, str):\\n        return False\\n    \\n    # AWS IAM Role ARN pattern\\n    # Format: arn:aws:iam::account-id:role/role-name\\n    # or: arn:aws:iam::account-id:role/path/role-name\\n    pattern = r'^arn:aws:iam::\\\\d{12}:role/[a-zA-Z0-9+=,.@\\\\-_/]+$'\\n    \\n    return bool(re.match(pattern, arn))\\n\\n\\n# Test cases\\nif __name__ == \\\"__main__\\\":\\n    # Valid ARNs\\n    valid_arns = [\\n        \\\"arn:aws:iam::123456789012:role/MyRole\\\",\\n        \\\"arn:aws:iam::123456789012:role/service-role/MyRole\\\",\\n        \\\"arn:aws:iam::999999999999:role/path/to/role-name\\\",\\n        \\\"arn:aws:iam::000000000000:role/Role_With-Special.Chars+123\\\",\\n    ]\\n    \\n    # Invalid ARNs\\n    invalid_arns = [\\n        \\\"arn:aws:iam::123456789012:user/MyUser\\\",  # user, not role\\n        \\\"arn:aws:iam::123456789012:role\\\",  # missing role name\\n        \\\"arn:aws:iam::12345:role/MyRole\\\",  # invalid account ID\\n        \\\"arn:aws:iam:us-east-1:123456789012:role/MyRole\\\",  # has region\\n        \\\"arn:aws:ec2::123456789012:instance/i-1234567890abcdef0\\\",  # wrong service\\n        \\\"invalid-arn\\\",  # not an ARN\\n        \\\"\\\",  # empty string\\n        None,  # None type\\n        123,  # non-string type\\n    ]\\n    \\n    print(\\\"Testing valid ARNs:\\\")\\n    for arn in valid_arns:\\n        result = validate_iam_role_arn(arn)\\n        print(f\\\"  {arn}: {result}\\\")\\n        assert result is True, f\\\"Expected True for {arn}\\\"\\n    \\n    print(\\\"\\\\nTesting invalid ARNs:\\\")\\n    for arn in invalid_arns:\\n        result = validate_iam_role_arn(arn)\\n        print(f\\\"  {arn}: {result}\\\")\\n        assert result is False, f\\\"Expected False for {arn}\\\"\\n    \\n    print(\\\"\\\\n\\u2713 All tests passed!\\\")\\n\",\n",
      "    \"score\": 9.0,\n",
      "    \"reasoning\": \"The solution correctly implements the core ARN validation requirements with a robust regex pattern that validates all essential components. The type checking, comprehensive test cases, and clear documentation demonstrate good software engineering practices. The main limitations are around edge cases for different AWS partitions and some nuanced AWS-specific path validation rules, but these don't affect the core functionality for standard AWS ARNs.\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Write a Python function that validates an AWS IAM role ARN string and returns True if valid, False otherwise\",\n",
      "      \"format\": \"python\",\n",
      "      \"solution_criteria\": \"Function must validate ARN format (arn:aws:iam::account-id:role/role-name), check account ID is numeric, ensure role name contains only valid characters, and handle edge cases like empty strings\"\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "with open('prompt_eval_dataset.json', 'r') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# run the evaluation\n",
    "results = run_eval(dataset)\n",
    "average_score = mean([result[\"score\"] for result in results])\n",
    "\n",
    "print(f\"Average score: {average_score}\")\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "print(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a023e724",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
